---
title: "foodb analysis"
output: html_document
date: "2025-02-19"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Packages inladen

```{r}
library(dplyr)
library(tidyr)
library(ggplot2)
library(compositions)
library(ggrepel)
library(broom)
library(tibble)
```

Data inladen

```{r}
foodb <- read.csv("C:\\Users\\bramd_finhsgu\\OneDrive - UGent\\Thesis\\Thesis_scripts\\Masterthesis_2425_Bram_Duthoo\\scripts\\foodb\\foodb_data.csv", header = TRUE)

foodb_nutrient <- foodb %>%
  filter(source_type == "Nutrient") %>%
  select(-source_type)

foodb_compound <- foodb %>%
  filter(source_type == "Compound") %>%
  select(-source_type)
```

foodb_compound omzetten in dataframe voor verdere bewerkingen + aanmaken food & source id lookup tabel voor interpretatie resultaten.

```{r}
# Stap 1: Creëer de eerste dataframe
recepten <- foodb_compound %>%
  select(orig_food_id, source_id, converted_value) %>%
  pivot_wider(names_from = source_id, values_from = converted_value, values_fn = list(converted_value = mean), values_fill = 0.00)

# Stap 2: Creëer de tweede dataframe
food_id_lookup <- foodb_compound %>%
  select(orig_food_id, food_id, orig_food_common_name) %>%
  distinct() %>%
  arrange(orig_food_id)

# Stap 3: Creëer de derde dataframe
source_id_lookup <- foodb_compound %>%
  select(source_id, orig_source_name, subklass) %>%
  distinct() %>%
  arrange(source_id)

# Toon de dataframes
print(recepten)
print(food_id_lookup)
print(source_id_lookup)

```

Data verkenning source_ids (alle verschillende compounds), distributie over alle food_id's (~recepten) indien sample size groot/relevant genoeg is. Grens ligt momenteel op 100, handmatig gekozen om veel recepten, met weinig nulwaardes, over te houden. Echter geen volledig correcte manier van compounds verwijderen, idee: a) alle compounds met in elk recept een nulwaarde verwijderen. b) Belang compounds bepalen binnenin een food_id (diersoort). d.w.z. dat compounds die binnenin een food_id voor bijna elk recept voorkomen belangrijk zijn om te behouden, ook al is het totaal aantal nulwaarden over alle food_id's heel groot. Compounds die over elke food_id een verdeling hebben van niet nul-waarden % die onder een drempel ligt worden verwijderd.



```{r}
# Vind de source_ids waarvan alle waarden in 'recepten' nul zijn
source_ids_all_zero <- recepten %>%
  select(-orig_food_id) %>%  # Exclude orig_food_id kolom
  summarise_all(~ all(. == 0)) %>%
  pivot_longer(everything(), names_to = "source_id", values_to = "all_zeros") %>%
  filter(all_zeros) %>%
  pull(source_id)

recepten <- recepten %>%
  select(-all_of(source_ids_all_zero)) 

# Voeg de food_id toe aan het recepten dataframe
recepten_with_food_id <- recepten %>%
  left_join(food_id_lookup, by = "orig_food_id")  # Voeg food_id toe

# Bereken per food_id en source_id het percentage niet-nulwaarden
source_food_distribution <- recepten_with_food_id %>%
  select(-orig_food_id, -orig_food_common_name) %>%  # Hou alleen food_id en source_id waardes
  pivot_longer(-food_id, names_to = "source_id", values_to = "value") %>%  # Maak lange tabel
  group_by(food_id, source_id) %>%
  summarise(non_zero_ratio = sum(value != 0) / n(), .groups = "drop") %>%
  pivot_wider(names_from = food_id, values_from = non_zero_ratio, values_fill = 0)  # Maak matrix

print(source_food_distribution)
```

```{r}
source_food_distribution_TeBehouden <- source_food_distribution %>%
  rowwise() %>%
  filter(
    # Voorwaarde 1: Minstens 2 van de 4 kolommen voldoen aan hun drempelwaarde
    sum(`334` >= 0.8, `483` >= 0.8, `506` >= 0.5, `549` >= 0.5) >= 2 |
    
    # Voorwaarde 2: Minstens 1 kolom >= 0.8 + 2 andere kolommen moeten een niet-nulwaarde hebben
    (max(`334`, `483`, `506`, `549`) >= 0.8 & sum(`334` != 0, `483` != 0, `506` != 0, `549` != 0) >= 3) |
    
    # Voorwaarde 3: Kolom 506 >= 0.53 en kolom 549 >= 0.3, of andersom
    (`506` >= 0.53 & `549` >= 0.3) | 
    (`549` >= 0.53 & `506` >= 0.3)
  ) %>%
  ungroup()

print(source_food_distribution_TeBehouden)

source_food_distribution_TeVerwijderen <- anti_join(
  source_food_distribution, 
  source_food_distribution_TeBehouden, 
  by = "source_id"
)

print(source_food_distribution_TeVerwijderen)
```

Poging 1 filteren source_id's

```{r}
# Bereken percentage nulwaarden per orig_food_id
percentage_nulwaarden <- recepten %>%
  mutate(total_values = rowSums(. != 0),  # Tel niet-nulwaarden
         total_columns = ncol(.) - 1,  # Aantal bron ID's (kolommen) minus orig_food_id
         percentage_nul = (total_columns - total_values) / total_columns * 100) %>%
  select(orig_food_id, percentage_nul)

print(percentage_nulwaarden)

# Groeperen en tellen van niet-nulwaarden per source_id
recepten_long <- recepten %>%
  pivot_longer(-orig_food_id, names_to = "source_id", values_to = "value") %>%
  group_by(source_id) %>%
  summarise(n_niet_nul = sum(value != 0),
            n_nul = sum(value == 0),
            .groups = "drop") %>%
  mutate(is_groot_genoeg = n_niet_nul >= 100)

# Split de data op basis van het aantal niet-nulwaarden
source_id_groot_genoeg <- recepten_long %>%
  filter(is_groot_genoeg)

source_id_te_weinig <- recepten_long %>%
  filter(!is_groot_genoeg)

recepten_long_full <- recepten %>%
  pivot_longer(-orig_food_id, names_to = "source_id", values_to = "value")

# Distributie voor source_id's met 25+ niet-nulwaarden
for (id in source_id_groot_genoeg$source_id) {
  p <- ggplot(recepten_long_full %>%
                filter(source_id == id),
              aes(x = value)) +
    geom_histogram(bins = 30, fill = "blue", alpha = 0.7) +
    ggtitle(paste("Distributie van source_id", id)) +
    xlab("Waarden") +
    ylab("Frequentie") +
    theme_minimal()
  
  print(p)  # Dit zorgt ervoor dat de plots correct getoond worden
}

# Statistieken voor source_id's met minder dan 25 niet-nulwaarden
stats_te_weinig <- source_id_te_weinig %>%
  mutate(mean_nulwaarden = n_nul / (n_nul + n_niet_nul) * 100)

# Bereken statistieken met de correcte dataset recepten_long_full
summary_stats <- recepten_long_full %>%
  filter(source_id %in% source_id_te_weinig$source_id) %>%
  group_by(source_id) %>%  # Groepeer per source_id
  summarise(
    mean_value = mean(value, na.rm = TRUE),
    median_value = median(value, na.rm = TRUE),
    q25 = quantile(value, 0.25, na.rm = TRUE),
    q75 = quantile(value, 0.75, na.rm = TRUE),
    .groups = "drop"  # Zorgt ervoor dat het resultaat geen gegroepeerd dataframe is
  )

print(stats_te_weinig)
print(summary_stats)
```

Poging 3 filteren source_id's

```{r}
# Definieer de gewenste orig_food_id's
selected_food_ids <- c(36014, 36015, 36016, 36017, 36018, 36021, 36022, 36023)
extra_food_ids <- c(421, 13803, 435, 13227, 13917, 23230, 372, 373, 927, 10953, 10088, 1244, 371, 1384, 13972, 5016, 928)

# Combineer beide lijsten
all_selected_food_ids <- c(selected_food_ids, extra_food_ids)

# Filter en verzamel unieke source_id's
unique_source_ids <- foodb_compound %>%
  filter(orig_food_id %in% selected_food_ids) %>%  # Houd alleen de geselecteerde orig_food_id's
  pull(source_id) %>%  # Haal de source_id kolom op
  unique()  # Selecteer alleen unieke waarden

unique_source_ids <- as.character(unique_source_ids)

# Filter de recepten dataframe op basis van de unieke source_id's
recepten_filtered <- recepten %>%
  select(orig_food_id, any_of(unique_source_ids)) %>%  # Behoud enkel de kolommen in unique_source_ids
  filter(orig_food_id %in% all_selected_food_ids) # Behoud alleen de rijen met de geselecteerde orig_food_id's
  #select_if(~ any(. != 0))

# Haal de kolomnamen op (exclusief orig_food_id)
source_ids <- colnames(recepten_filtered)[-1]

# Koppel de source_id's aan hun namen en gebruik de eerste naam voor elke source_id
source_names <- source_id_lookup %>%
  filter(source_id %in% source_ids) %>%  # Houd alleen de source_id's die in de kolommen staan
  group_by(source_id) %>%  # Groepeer op source_id
  summarise(orig_source_name = first(orig_source_name), .groups = "drop") %>%  # Neem de eerste naam
  arrange(source_id)  # Sorteer op source_id

# Print de resultante namen
print(source_names)

FOODB_MEAT <- recepten_filtered %>%
  pivot_longer(-orig_food_id, names_to = "source_id", values_to = "value")

```

toevoegen recept video:
- glutamic acid heeft enkel nulwaardes in database, vermoedelijk niet onderzocht i.p.v. een echte nulwaarde. Substituent vinden in literatuur. Tot dan heeft het toevoegen van MSG weinig zin aangezien de data niet op deze compound vergeleken kan worden.
- idem voor suiker compounds. Volgens patenten, video en literatuur wel belangrijke rol in smaakontwikkeling, dus waarden hiervoor zijn essentieel voor een correcte gebruik van resultaten.
- idem voor zout, is echter zoals MSG meer een smaakversterker/enhancer en draagt minder bij tot de essentie van de smaakontwikkeling.
- 

Visualizatie van het aantal recepten dat overeenkomt met het minimum aantal niet-nulwaarden %. Hulpmiddel bij evenwicht zoeken tussen genoeg recepten behouden en zo min mogelijk nulwaarden (probleem voor log-transformaties).

```{r}
# Stap 1: Filteren van source_id's met genoeg niet-nulwaarden
groot_genoeg_ids <- source_food_distribution_TeBehouden$source_id  # Veronderstel dat dit de correcte bron is

recepten_long_full <- recepten %>%
  pivot_longer(-orig_food_id, names_to = "source_id", values_to = "value")

# Stap 2: Bereken percentage niet-nulwaarden per orig_food_id
percentage_niet_nul <- recepten_long_full %>%
  filter(source_id %in% groot_genoeg_ids) %>%  # Beperk tot de relevante source_id's
  group_by(orig_food_id) %>%
  summarise(
    total_values = n(),  # Aantal componenten (source_id's)
    niet_nul_values = sum(value != 0, na.rm = TRUE),  # Aantal niet-nulwaarden
    percentage_niet_nul = niet_nul_values / total_values * 100,  # Percentage niet-nulwaarden
    .groups = "drop"
  )

# Stap 2: Groepeer per percentage_niet_nul en tel het aantal orig_food_ids per groep
cumulative_distribution <- percentage_niet_nul %>%
  group_by(percentage_niet_nul) %>% 
  summarise(count = n(), .groups = "drop") %>%  # Tel het aantal orig_food_ids per % niet-nulwaarde
  arrange(desc(percentage_niet_nul)) %>%  # Sorteer van hoog naar laag
  mutate(cumulative_count = cumsum(count)) %>%  # Cumulatieve som nemen
  mutate(cumulative_percentage = cumulative_count / sum(count) * 100)  # Omzetten naar %

# Stap 3: Debugging - Check of cumulatieve verdeling correct werkt
print(head(cumulative_distribution))  # Bekijk de eerste rijen om te zien of data klopt

# Stap 4: Visualiseer de cumulatieve verdeling
ggplot(cumulative_distribution, aes(x = percentage_niet_nul, y = cumulative_count)) +
  geom_line(color = "blue", size = 1) +
  geom_area(fill = "lightblue", alpha = 0.5) +
  scale_x_continuous(breaks = seq(0, 100, by = 5)) +  # X-as van 0% tot 100%
  labs(
    title = "Afnemende Cumulatieve Verdeler van Niet-Nulwaarden per Orig Food ID",
    x = "Percentage Niet-Nulwaarden",
    y = "Aantal Orig Food IDs met Minstens Dit Percentage"
  ) +
  theme_minimal()
```

Resultaat voordien toonde aan dat 80% een goed evenwichtspunt is. Hieronder is een functie die het aantal recepten dat overeenkomt met een minimum % van niet nulwaarden = 80%, in functie toont van hoe streng compounds worden weggefilterd (zie n_niet_nul bij bereken van alle distributies).

```{r}
library(tidyverse)

# Stap 1: Groeperen en tellen van niet-nulwaarden per source_id
recepten_long <- recepten %>%
  pivot_longer(-orig_food_id, names_to = "source_id", values_to = "value") %>%
  group_by(source_id) %>%
  summarise(n_niet_nul = sum(value != 0),  # Tel aantal niet-nulwaarden
            .groups = "drop")

# Stap 2: Genereer een reeks drempelwaarden voor n_niet_nul
drempels <- seq(10, max(recepten_long$n_niet_nul), by = 10)

# Stap 3: Iteratief filteren en opslaan hoeveel orig_food_id's een niet-nulpercentage ≥ 80% behouden
resultaten <- map_dfr(drempels, function(drempel) {
  
  # Filter source_id's met minstens 'drempel' niet-nulwaarden
  relevante_source_ids <- recepten_long %>%
    filter(n_niet_nul >= drempel) %>%
    pull(source_id)
  
  # Filter recepten_long_full om alleen relevante source_id's te behouden
  percentage_niet_nul <- recepten %>%
    pivot_longer(-orig_food_id, names_to = "source_id", values_to = "value") %>%
    filter(source_id %in% relevante_source_ids) %>%
    group_by(orig_food_id) %>%
    summarise(
      total_values = n(),  
      niet_nul_values = sum(value != 0, na.rm = TRUE),
      percentage_niet_nul = niet_nul_values / total_values * 100,
      .groups = "drop"
    )
  
  # Bepaal hoeveel orig_food_id's nog steeds een niet-nulpercentage ≥ 80% hebben
  aantal_boven_80 <- sum(percentage_niet_nul$percentage_niet_nul >= 80)
  
  tibble(n_niet_nul_drempel = drempel, aantal_orig_food_boven_80 = aantal_boven_80)
})

# Stap 4: Visualiseer het effect van de drempel op het aantal orig_food_id's met ≥80% niet-nulwaarden
ggplot(resultaten, aes(x = n_niet_nul_drempel, y = aantal_orig_food_boven_80)) +
  geom_line(color = "blue", size = 1) +
  geom_point(color = "red", size = 2) +
  labs(
    title = "Effect van n_niet_nul-drempel op het aantal orig_food_id's met ≥80% niet-nulwaarden",
    x = "Minimum aantal niet-nulwaarden per source_id",
    y = "Aantal orig_food_id's met ≥80% niet-nulwaarden"
  ) +
  theme_minimal()

```

Data filteren op basis van vorige resultaten voor outlier detecte en PCA. Nog eens orig_food_id filter nakijken in vscode script aangezien er nog sommige ongewenste id's doorkomen. Herbekijken na gebruik van andere source_id filtering.

```{r}
#extra_ids <- c("10005", "10102", "36014", "36015", "36016", "36017", "36018", "36019", "36020", "36021",
              # "36022", "36023", "36024", "13000", "13047", "17271", "17222", "10100", "10174", "13097",
              # "13227", "13976", "13978", "17099", "17107", "17113", "17119", "17125", "17131", "17137",
              # "17140", "17278", "23513", "13095", "13284", "13147", "13149", "13339", "17128", "17276",
              # "13838", "13841", "13844", "13847", "13859", "23623", "322", "323", "428", "13023",
              # "10072", "10121", "10187", "10219", "10228", "17110", "17142")  # Voeg hier je specifieke ID's toe

selected_orig_food_ids <- percentage_niet_nul %>%
  filter(percentage_niet_nul >= 60) %>%
  pull(orig_food_id)  # Haal de waarden op als een vector

# Stap 2: Filter recepten_long_full op deze orig_food_id's en groot_genoeg_ids
FOODB_MEAT <- recepten_long_full %>%
  filter(orig_food_id %in% selected_orig_food_ids & source_id %in% groot_genoeg_ids)

```

Als epsilon = min. waarde in dataset / 10. Controleren of dit klein genoeg is, + gevoeligheidsanalyse voor epsilon waarde.
ILR als standaard log-transformatie genomen, kan nog eens vergeleken worden met andere transformaties.


```{r}
# Stap 1: Data omzetten naar brede vorm (compositie per orig_food_id)
foodb_wide <- FOODB_MEAT %>%
  pivot_wider(names_from = source_id, values_from = value, values_fill = 0) %>%
  column_to_rownames("orig_food_id")  # Zet orig_food_id als rijnaam

# Stap 2: Vervang nullen door epsilon (klein getal om logproblemen te vermijden)
epsilon <- 1e-6
foodb_no_zeros <- foodb_wide %>%
  mutate(across(everything(), ~ ifelse(. == 0, epsilon, .)))

# Stap 3: Normaliseren (compositiesommen moeten gelijk zijn aan 1)
foodb_composition <- foodb_no_zeros %>%
  mutate(across(everything(), ~ . / rowSums(foodb_no_zeros, na.rm = TRUE)))

# Stap 4: ILR-transformatie uitvoeren
foodb_ilr <- compositions::ilr(foodb_composition)
```

Mahalanobis-afstanden & PCA hebben multivariate normaliteit als voorwaarde, deze voorwaarde moet beter worden nagegaan om geloofwaardigheid resultaten in te schatten.

```{r}
# Stap 5: Outlierdetectie met Mahalanobis-afstanden
#mahal_distances <- mahalanobis(foodb_ilr, colMeans(foodb_ilr, na.rm = TRUE), cov(foodb_ilr, use = "pairwise.complete.obs"))
#threshold <- qchisq(0.975, df = ncol(foodb_ilr))  # 97.5% grenswaarde
#outliers <- which(mahal_distances > threshold)

# Lijst van outliers
#list_of_outliers <- rownames(foodb_ilr)[outliers]
#print("Outliers gevonden op basis van Mahalanobis-afstanden:")
#print(list_of_outliers)

# Stap 6: PCA uitvoeren op ILR-getransformeerde data
pca_model <- prcomp(foodb_ilr, scale. = FALSE)

pca_df <- as.data.frame(pca_model$x) %>%
  rownames_to_column("orig_food_id") %>%
  mutate(orig_food_id = as.character(orig_food_id)) %>%  # Zorg dat het character is
  left_join(food_id_lookup %>% mutate(orig_food_id = as.character(orig_food_id)), by = "orig_food_id")  # Voeg food_id toe voor kleur

# Stap 8: PCA plotten (kleur per food_id)
ggplot(pca_df, aes(x = PC1, y = PC2, color = as.factor(food_id), label = orig_food_id)) +
  geom_point(size = 3, alpha = 0.8) +
  geom_text_repel(size = 3) +
  labs(title = "PCA van Meat Composition Data",
       x = "Principal Component 1",
       y = "Principal Component 2",
       color = "Food ID") +
  theme_minimal()

```
Food ID 334 = kip, 483 = schaap, 506 = rund, 549 = varken.
