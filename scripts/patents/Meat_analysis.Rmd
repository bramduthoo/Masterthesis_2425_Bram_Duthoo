---
title: "Meat flavor analysis"
output: html_document
date: "`r format(Sys.Date())`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
```

# Data processing & exploratie

Deze sectie heeft als doel de compositionele data voor te bereiden voor log-ratio transformaties. Hiervoor worden alle parts (groepen in een compositie) van de compositie bekeken, geïnterpreteerd en omgezet in betere structuur voor verdere analyses. Daarna wordt de data geanalyseerd op aanwezigheid van nulwaardes aangezien deze problematisch zijn in log-ratio transformaties, en besproken hoe ermee omgegaan zal worden. Ten slotte worden outliers gedecteerd, en indien nodig verwijderd zodat we eindigen met een dataset geschikt voor log-ratio transformaties

## Data inladen

Alle nodige packages worden hier ingeladen.

```{r libraries inladen}
library(DBI)
library(RSQLite)
library(stats)
library(tidyverse)
library(compositions) 
library(rlang)
library(MASS)       
library(ggplot2)
library(ggfortify)   
library(dplyr)
library(tidyr) 
```

De database die hier gebruikt wordt is opgebouwd door in patenten (met als onderwerp 'synthetiseren van artificiële vleessmaken') op zoek te gaan voorbeeldrecepten die een bepaalde vleessmaak nabootsen. Als alle recepten voorgesteld zouden worden als een compositie van alle ingrediënten zullen er later problemen ontstaan tijdens de analyse aangezien het aantal parts te groot is: 71. Hierdoor worden alle ingrediënten samengevat door hun chemische groepen zoals suikers, aminozuren, ribonucleotides, etc. en worden recepten gezien als een compositie hiervan. Deze beslissing wordt ook gemotiveerd door de informatie in de patenten aangezien in de beschrijving van de synthetische vleessmaak enkel over chemische groepen wordt gesproken, en enkel in recepten specifieke ingrediënten worden gebruikt. Vaak zijn deze ingrediënten dan ook de meest economische optie binnenin de categorie, en verliest het veralgemenen van ingrediënten naar chemische groepen geen relevante informatie voor het synthetiseren van vleessmaken.

```{r database inladen}
db <- dbConnect(RSQLite::SQLite(), 
                "C:/Users/bramd_finhsgu/OneDrive - UGent/Thesis/Thesis_bestanden/recipes9.db")

recipes            <- dbReadTable(db, "recipes")
chemical_groups    <- dbReadTable(db, "chemical_groups")
ingredients        <- dbReadTable(db, "ingredients")
recipe_ingredients <- dbReadTable(db, "recipe_ingredients")


data_grouped <- recipe_ingredients %>%
  left_join(ingredients, by = c("ingredient_id" = "id")) %>%  # Voeg ingrediënten toe
  left_join(chemical_groups, by = c("chemical_group_id" = "id")) %>%  # Voeg chemische groepen toe
  group_by(recipe_id, group_name) %>%
  summarise(total_quantity = sum(total_quantity, na.rm = TRUE), .groups = "drop") %>%
  pivot_wider(names_from = group_name, values_from = total_quantity, values_fill = 0) %>%
  # LET OP: we gebruiken expliciet dplyr::select(...) om conflicts te vermijden
  left_join(
    recipes %>% dplyr::select(id, recipe_name, cooking_time, temperature, flavor_label, preparation), 
    by = c("recipe_id" = "id")
  ) %>%
  # Ook hier expliciet dplyr::select(...):
  dplyr::select(recipe_name, cooking_time, temperature, flavor_label, preparation, everything(), -recipe_id)

print("Header van data_grouped:")
print(head(data_grouped))
```

## Nullen controleren

De aanwezigheid van nullen in compositionele data is een probleem aangezien het niet compatibel is met log-ratio transformaties. In compositionele data worden er 2 soorten nullen onderscheiden: rounded zeroes & structural zeroes. Rounded zeroes zijn de nullen die door meetfouten of processfouten niet zijn opgenomen of verwerkt, maar er wordt wel een waarde verwacht, structural zeroes zijn dan de nullen die effectief een nul representeren. Er bestaan verschillende manieren om om te gaan met rounded zeroes zoals imputatie, KNN, Bayesian-multiplicative replacement strategy, etc. aangezien er een waarde verwacht wordt en de nul vervangen kan worden. Voor structural zeroes gaat dit helaas niet zo makkelijk aangezien de 0 verwacht wordt en betekenis heeft. Tot op heden zijn er nog geen algemeen aanvaarde manieren om te kunnen omgaan met structurele nullen.

*De nullen in onze data zijn structurele nullen door de aard van recepten, een afwezigheid van een component betekent ook effectief dat deze component afwezig hoort te zijn. Doordat er dus nog geen manieren zijn om hiermee om te gaan wordt er vaak voorgesteld in papers om een methode voor rounded zeroes toe te passen die aansluit bij de data en onderzoeksvraag. In dit geval lijkt imputatie (vervangen van 0 met een kleine waarde) me geschikt, sinds de definitie van structurele nullen me niet lijkt over te slaan naar smaakperceptie. Er bestaan smaaktresholds en "merkbare verschillen", wat gebruikt zou kunnen worden als excuus voor het toelaten van een heel kleine waarde.*
*Verschillende imputatie modellen testen en vergelijken via procrustes correlaties en gevoeligheidsanalyses, potentieel andere methodes zoeken/ontwikkelen die passen in de context van smaak*

```{r nullen inspecteren}
excluded_columns <- c("recipe_name", "cooking_time", "temperature", "preparation", "flavor_label")
percent_nulls <- data_grouped %>%
  summarise(across(-all_of(excluded_columns), ~ mean(. == 0, na.rm = TRUE) * 100))

result_table <- data.frame(
  Kolomnaam = colnames(percent_nulls),
  Percentage_Nullen = as.numeric(percent_nulls[1, ])
)
print(result_table)
```

Het is beter bij methodes zoals imputatie om het aantal nullen te vervangen te minimaliseren zodat de data zo min mogelijk wordt verstoord. Hiervoor is het relevant om te kijken naar de aanwezigheid van nullen in de data.
Zoals af te lezen is van de tabel, bevat de data heel veel nullen. De parts smaakenhancer, zuren, extra en volatiles bestaan uit minstens 50% nullen en worden in de patenten beschreven als aanvullend en niet-essentieel/betrokken in de synthese van de vleessmaak en zouden gefilterd kunnen worden van de dataset om het nulprobleem gedeeltelijk op te lossen en enkel te focussen op relevante data voor vleessmaak.
Daarnaast zijn er 
*Voordat deze parts worden gefilterd of samengevoegd, nog eens het effect hiervan bekijken met supervised logratio variable selection dat ons toelaat te quantificeren hoeveel variantie we zouden verliezen tov de gehele dataset. Gebruiken als backup voor statement dat deze categoriën toch niet veel bijdragen.*

## Outlier detectie

Outlier detectie voor compositionele data met strcuturele nullen:
Templ, M., Hron, K., & Filzmoser, P. (2016). Exploratory tools for outlier detection in compositional data with structural zeros. Journal of Applied Statistics, 44(4), 734–752. https://doi.org/10.1080/02664763.2016.1182135
*Nog geen functies hiervoor, voor deze workflow te integreren in een functie -> nadenken of deze detectie wel geschikt is als de nulwaarden toch geïmputeerd zullen worden. Het klinkt veelbelovend, maar potentieel overbodig als de nullen toch geïmputeerd worden.*

Indien we verder werken met de imputaties en gefilterde dataset:
Mahalanobis-afstanden is de standaard om outliers te detecteren in compositionele data 
*normaliteit 

```{r outliers detecteren}
data_preprocessed <- data_grouped %>%
  dplyr::select(-c(recipe_name, cooking_time, temperature, flavor_label, preparation)) %>%
  mutate(Carrier = Water + Vetten) %>%  
  dplyr::select(-c(Water, Vetten, Zuren, extra, Volatiles, Smaakenhancer))

epsilon <- 1e-3
data_no_zeros <- data_preprocessed %>%
  mutate(across(everything(), ~ ifelse(. == 0, epsilon, .)))

data_normalized <- data_no_zeros %>%
  mutate(across(everything(), ~ . / rowSums(data_no_zeros, na.rm = TRUE)))

data_ilr <- data_normalized %>%
  compositions::ilr()

mahal_distances <- mahalanobis(data_ilr, colMeans(data_ilr, na.rm = TRUE), cov(data_ilr, use = "pairwise.complete.obs"))
threshold <- qchisq(0.975, df = ncol(data_ilr))  
outliers <- which(mahal_distances > threshold)  

list_of_outliers <- outliers
print("Outliers gevonden op basis van Mahalanobis-afstanden:")
print(list_of_outliers)

ilr_df <- as.data.frame(data_ilr)
for (i in seq_along(ilr_df)) {
  col_name <- colnames(ilr_df)[i]  
  plot <- ggplot(ilr_df, aes(x = !!sym(col_name))) +
    geom_histogram(bins = 30, fill = "blue", alpha = 0.7) +
    labs(title = paste("Histogram van ILR-component:", col_name)) +
    theme_minimal()
  print(plot)
}
```
*Mahalanobis afstanden vereist multivariate normaliteit, potentieel betere alternatieven vinden. Imputaties hebben wel een grote invloed, extra bevestiging dat gevoeligheidstest nulvervanging vereist is.*

extra: Verdelingen hebben gelijkaardig bereik, geen nood aan gewichten dus.

# Data analyse

Om de onderzoeksvraag te kunnen beantwoorden 
-LRA: PCA analysis op log ratio's. visualizatie clusters tussen verschillende vleessmaken, loadings interpreteren, 
-(Ward) Cluster analysis: visualizatie, maar meer gefocust op welke groepen chemisch op mekaar lijken
- Supervised logratio variable selection: Selecteert logratio’s die zowel de variabiliteit in de data verklaren als de smaaklabels goed onderscheiden
- regressiemodellen: belangrijkste logratio's uit vorige stap gebruiken. Initieel LDA, dan SVM of RF proberen. Ook eens vleessmaak label als verklarende variabele om te zien hoeveel variatie dit in de compositie bevat

extra: 
1. onderzoeken of clusters in pca niet gewoon de variatie tussen patenten weerspiegelt of echt patronen in vleessmaak vastlegt. Potentieel compositiesubsets maken per patent en log/ratio variaties hierin vergelijken met die van de gehele dataset?
2. gevoeligheidsanalyse, omdat de smaaklabels uit patenten komen en mogelijk inconsistent zijn. Clustering op een unsupervised methode checken voor outliers.